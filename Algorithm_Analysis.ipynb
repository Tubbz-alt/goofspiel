{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some important references to understand what is happening underneath:\n",
    "\n",
    "[1] For transforming game to equivalent turn-based https://github.com/deepmind/open_spiel/blob/master/open_spiel/game_transforms/turn_based_simultaneous_game.cc)\n",
    "\n",
    "[2] For RL Environment https://github.com/deepmind/open_spiel/blob/master/open_spiel/python/rl_environment.py\n",
    "\n",
    "[3] For the game and state encodings https://github.com/deepmind/open_spiel/blob/master/open_spiel/games/goofspiel.cc\n",
    "\n",
    "[4] For policy https://github.com/deepmind/open_spiel/blob/master/open_spiel/python/policy.py\n",
    "\n",
    "Some examples\n",
    "\n",
    "[5] https://github.com/deepmind/open_spiel/tree/master/open_spiel/python/examples\n",
    "--> tic_tac_toe_qlearner.py, kuhn_nfsp.py, kuhn_policy_gradient.py, kuhn_poker_cfr.py\n",
    "\n",
    "Algorithms\n",
    "\n",
    "[6] https://github.com/deepmind/open_spiel/tree/master/open_spiel/python/algorithms\n",
    "--> tabular_qlearner.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps to Take: (Update 22.06.2020)\n",
    "\n",
    "Mark as (x) when done.\n",
    "\n",
    "* Change to turnbased. (x)\n",
    "* Adapt tabular q-learn. (x)\n",
    "* Understand and use rl environment. (x)\n",
    "* Try to get the learned policy as a table of probabilities.\n",
    "* Evaluate the the policy against random and other policies.\n",
    "* Try to implement more algorithms.\n",
    "* Compare different algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2019 DeepMind Technologies Ltd. All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"Tabular Q-Learner example on Tic Tac Toe.\n",
    "\n",
    "Two Q-Learning agents are trained by playing against each other. Then, the game\n",
    "can be played against the agents from the command line.\n",
    "\n",
    "After about 10**5 training episodes, the agents reach a good policy: win rate\n",
    "against random opponents is around 99% for player 0 and 92% for player 1.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "from absl import app\n",
    "from absl import flags\n",
    "import numpy as np\n",
    "from six.moves import input\n",
    "from six.moves import range\n",
    "\n",
    "from open_spiel.python import rl_environment\n",
    "from open_spiel.python.algorithms import random_agent\n",
    "from open_spiel.python.algorithms import tabular_qlearner\n",
    "import pyspiel\n",
    "\n",
    "# For gradient based methods\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "# For policy gradient\n",
    "from open_spiel.python import policy\n",
    "from open_spiel.python.algorithms import exploitability\n",
    "from open_spiel.python.algorithms import policy_gradient\n",
    "\n",
    "# For NFSP\n",
    "from open_spiel.python.algorithms import exploitability\n",
    "from open_spiel.python.algorithms import nfsp\n",
    "\n",
    "# For CFR\n",
    "from open_spiel.python.algorithms import cfr\n",
    "from open_spiel.python.algorithms import expected_game_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed for using logging in Notebook\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logging.debug(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Later modify it for goofspiel.\n",
    "\n",
    "def pretty_board(time_step):\n",
    "    \"\"\"Returns the board in `time_step` in a human readable format.\"\"\"\n",
    "    info_state = time_step.observations[\"info_state\"][0]\n",
    "    x_locations = np.nonzero(info_state[9:18])[0]\n",
    "    o_locations = np.nonzero(info_state[18:])[0]\n",
    "    board = np.full(3 * 3, \".\")\n",
    "    board[x_locations] = \"X\"\n",
    "    board[o_locations] = \"0\"\n",
    "    board = np.reshape(board, (3, 3))\n",
    "    return board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changed the format to print more user friendly on lines 8 and 12 the addition and subtraction by 1.\n",
    "def command_line_action(time_step):\n",
    "    \"\"\"Gets a valid action from the user on the command line.\"\"\"\n",
    "    current_player = time_step.observations[\"current_player\"]\n",
    "    legal_actions = time_step.observations[\"legal_actions\"][current_player]\n",
    "    cards_actions = [x+1 for x in legal_actions]\n",
    "    action = -1\n",
    "    while action not in legal_actions:\n",
    "        print(\"Choose an card to play from your hand {}:\".format(cards_actions))\n",
    "        sys.stdout.flush()\n",
    "        action_str = input()\n",
    "        try:\n",
    "            action = int(action_str) - 1\n",
    "        except ValueError:\n",
    "            continue\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_against_random_bots(env, trained_agents, random_agents, num_episodes):\n",
    "    \"\"\"Evaluates `trained_agents` against `random_agents` for `num_episodes`.\"\"\"\n",
    "    wins = np.zeros(2)\n",
    "    for player_pos in range(2):\n",
    "        if player_pos == 0:\n",
    "            cur_agents = [trained_agents[0], random_agents[1]]\n",
    "        else:\n",
    "            cur_agents = [random_agents[0], trained_agents[1]]\n",
    "        for _ in range(num_episodes):\n",
    "            time_step = env.reset()\n",
    "            while not time_step.last():\n",
    "                player_id = time_step.observations[\"current_player\"]\n",
    "                agent_output = cur_agents[player_id].step(time_step, is_evaluation=True)\n",
    "                time_step = env.step([agent_output.action])\n",
    "            if time_step.rewards[player_pos] > 0:\n",
    "                wins[player_pos] += 1\n",
    "    return wins / num_episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is some of the algorithms being used, be careful to run only one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabular Q Learning\n",
    "This is the training of tabular Q-Learning agents.\n",
    "\n",
    "You can select if the algorithm should use observation tensor of information state tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = pyspiel.load_game_as_turn_based('goofspiel(imp_info=False,num_cards=4)')\n",
    "num_players = 2\n",
    "num_cards = 4\n",
    "\n",
    "# if observation_type == rl_environment.ObservationType.OBSERVATION then uses observation tensor however,\n",
    "# currently not working as we load the game beforehand as turn based. Maybe checking the code for game\n",
    "# transforms might help with using observation tensor. Still might not be necessary.\n",
    "env = rl_environment.Environment(game,observation_type=None)\n",
    "num_actions = env.action_spec()[\"num_actions\"]\n",
    "\n",
    "agents = [\n",
    "    tabular_qlearner.QLearner(player_id=idx, num_actions=num_actions)\n",
    "    for idx in range(num_players)\n",
    "]\n",
    "\n",
    "# random agents for evaluation\n",
    "random_agents = [\n",
    "    random_agent.RandomAgent(player_id=idx, num_actions=num_actions)\n",
    "    for idx in range(num_players)\n",
    "]\n",
    "\n",
    "# 1. Train the agents\n",
    "training_episodes = int(2e4)\n",
    "for cur_episode in range(training_episodes):\n",
    "    if cur_episode % int(1e4) == 0:\n",
    "        win_rates = eval_against_random_bots(env, agents, random_agents, 1000)\n",
    "        logging.info(\"Starting episode %s, win_rates %s\", cur_episode, win_rates)\n",
    "    time_step = env.reset()\n",
    "    while not time_step.last():\n",
    "        player_id = time_step.observations[\"current_player\"]\n",
    "        agent_output = agents[player_id].step(time_step)\n",
    "        time_step = env.step([agent_output.action])\n",
    "\n",
    "    # Episode is over, step all agents with final info state.\n",
    "    for agent in agents:\n",
    "        agent.step(time_step)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient - To Be Updated - Currently works\n",
    "\n",
    "### Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_integer(\"num_episodes\", int(2e4), \"Number of train episodes.\")\n",
    "flags.DEFINE_integer(\"eval_every\", int(1e4), \"Eval agents every x episodes.\")\n",
    "flags.DEFINE_enum(\"loss_str\", \"rpg\", [\"rpg\", \"qpg\", \"rm\"], \"PG loss to use.\")\n",
    "'''\n",
    "\n",
    "num_episodes = int(2e4)\n",
    "eval_every = int(1e4)\n",
    "loss_str = \"rpg\"\n",
    "\n",
    "class PolicyGradientPolicies(policy.Policy):\n",
    "    \"\"\"Joint policy to be evaluated.\"\"\"\n",
    "\n",
    "    def __init__(self, env, nfsp_policies):\n",
    "        game = env.game\n",
    "        player_ids = [0, 1]\n",
    "        super(PolicyGradientPolicies, self).__init__(game, player_ids)\n",
    "        self._policies = nfsp_policies\n",
    "        self._obs = {\"info_state\": [None, None], \"legal_actions\": [None, None]}\n",
    "\n",
    "    def action_probabilities(self, state, player_id=None):\n",
    "        cur_player = state.current_player()\n",
    "        legal_actions = state.legal_actions(cur_player)\n",
    "\n",
    "        self._obs[\"current_player\"] = cur_player\n",
    "        self._obs[\"info_state\"][cur_player] = (\n",
    "            state.information_state_tensor(cur_player))\n",
    "        self._obs[\"legal_actions\"][cur_player] = legal_actions\n",
    "\n",
    "        info_state = rl_environment.TimeStep(\n",
    "            observations=self._obs, rewards=None, discounts=None, step_type=None)\n",
    "\n",
    "        p = self._policies[cur_player].step(info_state, is_evaluation=True).probs\n",
    "        prob_dict = {action: p[action] for action in legal_actions}\n",
    "        return prob_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Training and interactive human playing at the same time.\n",
    "It really learns a probabilistic policy. Later try to seperate the training and interactive playing.\n",
    "tf.Session() should stay open for playing against a human.\n",
    "Also try to return the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = pyspiel.load_game_as_turn_based('goofspiel(imp_info=False,num_cards=4)')\n",
    "num_players = 2\n",
    "\n",
    "env_configs = {\"players\": num_players}\n",
    "env = rl_environment.Environment(game, **env_configs)\n",
    "info_state_size = env.observation_spec()[\"info_state\"][0]\n",
    "num_actions = env.action_spec()[\"num_actions\"]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # pylint: disable=g-complex-comprehension\n",
    "    agents = [\n",
    "        policy_gradient.PolicyGradient(\n",
    "            sess,\n",
    "            idx,\n",
    "            info_state_size,\n",
    "            num_actions,\n",
    "            loss_str=loss_str,\n",
    "            hidden_layers_sizes=(128,)) for idx in range(num_players)\n",
    "    ]\n",
    "    expl_policies_avg = PolicyGradientPolicies(env, agents)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for ep in range(num_episodes):\n",
    "\n",
    "        if (ep + 1) % eval_every == 0:\n",
    "            losses = [agent.loss for agent in agents]\n",
    "            expl = exploitability.exploitability(env.game, expl_policies_avg)\n",
    "            msg = \"-\" * 80 + \"\\n\"\n",
    "            msg += \"{}: {}\\n{}\\n\".format(ep + 1, expl, losses)\n",
    "            logging.info(\"%s\", msg)\n",
    "\n",
    "        time_step = env.reset()\n",
    "        while not time_step.last():\n",
    "            player_id = time_step.observations[\"current_player\"]\n",
    "            agent_output = agents[player_id].step(time_step)\n",
    "            action_list = [agent_output.action]\n",
    "            time_step = env.step(action_list)\n",
    "\n",
    "        # Episode is over, step all agents with final info state.\n",
    "        for agent in agents:\n",
    "            agent.step(time_step)\n",
    "            \n",
    "\n",
    "            \n",
    "    # 2. Play from the command line against the trained agent.\n",
    "    human_player = 1\n",
    "    while True:\n",
    "        logging.info(\"You are playing as %s\", \"ID 1\" if human_player else \"ID 0\")\n",
    "        time_step = env.reset()\n",
    "        while not time_step.last():\n",
    "            player_id = time_step.observations[\"current_player\"]\n",
    "            if player_id == human_player:\n",
    "                agent_out = agents[human_player].step(time_step, is_evaluation=True)\n",
    "                #logging.info(\"\\n%s\", agent_out.probs)\n",
    "                #logging.info(\"\\n%s\", pretty_board(time_step))\n",
    "                action = command_line_action(time_step)\n",
    "            else:\n",
    "                agent_out = agents[1 - human_player].step(time_step, is_evaluation=True)\n",
    "                logging.info(\"\\n%s\", agent_out.probs)\n",
    "                action = agent_out.action\n",
    "                logging.info('Agent played: {}'.format(action+1))\n",
    "\n",
    "\n",
    "            logging.info('Player ID: %d', player_id)\n",
    "            #print(time_step.observations['info_state'][0])\n",
    "            #print(time_step.observations['info_state'][1])\n",
    "            #print(len(time_step.observations['info_state'][0]))\n",
    "            #print(env.observation_spec())\n",
    "\n",
    "            state = time_step.observations['info_state'][player_id]\n",
    "            state = np.asarray(state)\n",
    "\n",
    "            P_ob = np.where(state[points_ob_b:points_op_b] == 1)[0][0]\n",
    "            P_op = np.where(state[points_op_b:seq_b] == 1)[0][0]\n",
    "            logging.info('Points: P%d = %d P%d = %d',player_id, P_ob, 0 if player_id == 1 else 1, P_op)\n",
    "\n",
    "            which = num_cards - np.sum(state[np.size(state) - num_cards:])\n",
    "            curr = np.where(state[int(seq_b + which*num_cards): int(seq_b + (which+1)*num_cards)] == 1)[0][0] + 1\n",
    "            logging.info('Point Card (Middle Card): %d', curr )\n",
    "\n",
    "            # Can reach the current state of the game.\n",
    "            curr_state = env.get_state\n",
    "            print(curr_state)\n",
    "\n",
    "            time_step = env.step([action])\n",
    "\n",
    "    #logging.info(\"\\n%s\", pretty_board(time_step))\n",
    "\n",
    "        logging.info(\"End of game!\")\n",
    "        if time_step.rewards[human_player] > 0:\n",
    "            logging.info(\"You win\")\n",
    "        elif time_step.rewards[human_player] < 0:\n",
    "            logging.info(\"You lose\")\n",
    "        else:\n",
    "            logging.info(\"Draw\")\n",
    "        # Switch order of players\n",
    "        human_player = 1 - human_player"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Fictious Self-Play - To Be Updated - Currently Not Working\n",
    "\n",
    "### Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NFSPPolicies(policy.Policy):\n",
    "    \"\"\"Joint policy to be evaluated.\"\"\"\n",
    "\n",
    "    def __init__(self, env, nfsp_policies, mode):\n",
    "        game = env.game\n",
    "        player_ids = [0, 1]\n",
    "        super(NFSPPolicies, self).__init__(game, player_ids)\n",
    "        self._policies = nfsp_policies\n",
    "        self._mode = mode\n",
    "        self._obs = {\"info_state\": [None, None], \"legal_actions\": [None, None]}\n",
    "\n",
    "    def action_probabilities(self, state, player_id=None):\n",
    "        cur_player = state.current_player()\n",
    "        legal_actions = state.legal_actions(cur_player)\n",
    "\n",
    "        self._obs[\"current_player\"] = cur_player\n",
    "        self._obs[\"info_state\"][cur_player] = (\n",
    "            state.information_state_tensor(cur_player))\n",
    "        self._obs[\"legal_actions\"][cur_player] = legal_actions\n",
    "\n",
    "        info_state = rl_environment.TimeStep(\n",
    "            observations=self._obs, rewards=None, discounts=None, step_type=None)\n",
    "\n",
    "        with self._policies[cur_player].temp_mode_as(self._mode):\n",
    "            p = self._policies[cur_player].step(info_state, is_evaluation=True).probs\n",
    "        prob_dict = {action: p[action] for action in legal_actions}\n",
    "        return prob_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = pyspiel.load_game_as_turn_based('goofspiel(imp_info=False,num_cards=4)')\n",
    "num_players = 2\n",
    "\n",
    "env_configs = {\"players\": num_players}\n",
    "env = rl_environment.Environment(game, **env_configs)\n",
    "info_state_size = env.observation_spec()[\"info_state\"][0]\n",
    "num_actions = env.action_spec()[\"num_actions\"]\n",
    "\n",
    "hidden_layers_sizes = [int(l) for l in FLAGS.hidden_layers_sizes]\n",
    "kwargs = {\n",
    "    \"replay_buffer_capacity\": FLAGS.replay_buffer_capacity,\n",
    "    \"epsilon_decay_duration\": FLAGS.num_train_episodes,\n",
    "    \"epsilon_start\": 0.06,\n",
    "    \"epsilon_end\": 0.001,\n",
    "}\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # pylint: disable=g-complex-comprehension\n",
    "    agents = [\n",
    "        nfsp.NFSP(sess, idx, info_state_size, num_actions, hidden_layers_sizes,\n",
    "                  FLAGS.reservoir_buffer_capacity, FLAGS.anticipatory_param,\n",
    "                  **kwargs) for idx in range(num_players)\n",
    "    ]\n",
    "    expl_policies_avg = NFSPPolicies(env, agents, nfsp.MODE.average_policy)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for ep in range(FLAGS.num_train_episodes):\n",
    "        if (ep + 1) % FLAGS.eval_every == 0:\n",
    "            losses = [agent.loss for agent in agents]\n",
    "            logging.info(\"Losses: %s\", losses)\n",
    "            expl = exploitability.exploitability(env.game, expl_policies_avg)\n",
    "            logging.info(\"[%s] Exploitability AVG %s\", ep + 1, expl)\n",
    "            logging.info(\"_____________________________________________\")\n",
    "\n",
    "        time_step = env.reset()\n",
    "        while not time_step.last():\n",
    "            player_id = time_step.observations[\"current_player\"]\n",
    "            agent_output = agents[player_id].step(time_step)\n",
    "            action_list = [agent_output.action]\n",
    "            time_step = env.step(action_list)\n",
    "\n",
    "          # Episode is over, step all agents with final info state.\n",
    "        for agent in agents:\n",
    "            agent.step(time_step)\n",
    "            \n",
    "            \n",
    "            \n",
    "    # 2. Play from the command line against the trained agent.\n",
    "    human_player = 1\n",
    "    while True:\n",
    "        logging.info(\"You are playing as %s\", \"ID 1\" if human_player else \"ID 0\")\n",
    "        time_step = env.reset()\n",
    "        while not time_step.last():\n",
    "            player_id = time_step.observations[\"current_player\"]\n",
    "            if player_id == human_player:\n",
    "                agent_out = agents[human_player].step(time_step, is_evaluation=True)\n",
    "                #logging.info(\"\\n%s\", agent_out.probs)\n",
    "                #logging.info(\"\\n%s\", pretty_board(time_step))\n",
    "                action = command_line_action(time_step)\n",
    "            else:\n",
    "                agent_out = agents[1 - human_player].step(time_step, is_evaluation=True)\n",
    "                logging.info(\"\\n%s\", agent_out.probs)\n",
    "                action = agent_out.action\n",
    "                logging.info('Agent played: {}'.format(action+1))\n",
    "\n",
    "\n",
    "            logging.info('Player ID: %d', player_id)\n",
    "            #print(time_step.observations['info_state'][0])\n",
    "            #print(time_step.observations['info_state'][1])\n",
    "            #print(len(time_step.observations['info_state'][0]))\n",
    "            #print(env.observation_spec())\n",
    "\n",
    "            state = time_step.observations['info_state'][player_id]\n",
    "            state = np.asarray(state)\n",
    "\n",
    "            P_ob = np.where(state[points_ob_b:points_op_b] == 1)[0][0]\n",
    "            P_op = np.where(state[points_op_b:seq_b] == 1)[0][0]\n",
    "            logging.info('Points: P%d = %d P%d = %d',player_id, P_ob, 0 if player_id == 1 else 1, P_op)\n",
    "\n",
    "            which = num_cards - np.sum(state[np.size(state) - num_cards:])\n",
    "            curr = np.where(state[int(seq_b + which*num_cards): int(seq_b + (which+1)*num_cards)] == 1)[0][0] + 1\n",
    "            logging.info('Point Card (Middle Card): %d', curr )\n",
    "\n",
    "            # Can reach the current state of the game.\n",
    "            curr_state = env.get_state\n",
    "            print(curr_state)\n",
    "\n",
    "            time_step = env.step([action])\n",
    "\n",
    "    #logging.info(\"\\n%s\", pretty_board(time_step))\n",
    "\n",
    "        logging.info(\"End of game!\")\n",
    "        if time_step.rewards[human_player] > 0:\n",
    "            logging.info(\"You win\")\n",
    "        elif time_step.rewards[human_player] < 0:\n",
    "            logging.info(\"You lose\")\n",
    "        else:\n",
    "            logging.info(\"Draw\")\n",
    "        # Switch order of players\n",
    "        human_player = 1 - human_player"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing Against a Human"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play Against a Human\n",
    "\n",
    "For acquiring important information from the time_step.observations dictionary you can select the info_state, then select the necessary information.\n",
    "\n",
    "n = number of cards\n",
    "p = 2, number of players\n",
    "\n",
    "If using information state tensor with imp_info = False:\n",
    "\n",
    "First 2 bits show which player is the current player.\n",
    "\n",
    "Next 2 bits show which player the current observation belongs to.\n",
    "\n",
    "(See line 178 in [1])\n",
    "\n",
    "Next $ \\frac{n \\cdot (n+1)}{2} + 1 $ bits show the points of the player observing.\n",
    "\n",
    "Next $ \\frac{n \\cdot (n+1)}{2} + 1 $ bits show the points of the opponent.\n",
    "\n",
    "Next $ n^{2} $ bits show the sequence of the point cards, namely the cards that are opened in the middle.\n",
    "\n",
    "Last $ n \\cdot p $ bits show the hand of each player ordered from the perspective of the observing player (observing player has the first n bits and opponent has the last n bits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# important indices:\n",
    "\n",
    "# Usage: ob for observing, op for opponent, b for begining index, cu for current\n",
    "\n",
    "cu_player_b = 0\n",
    "ob_player_b = num_players\n",
    "points_ob_b = 2*num_players\n",
    "points_op_b = points_ob_b + int(num_cards*(num_cards+1)/2+1)\n",
    "seq_b = points_op_b + int(num_cards*(num_cards+1)/2+1)\n",
    "hand_ob_b = seq_b + num_cards**2\n",
    "hand_op_b = hand_ob_b + num_cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2. Play from the command line against the trained agent.\n",
    "human_player = 1\n",
    "while True:\n",
    "    logging.info(\"You are playing as %s\", \"ID 1\" if human_player else \"ID 0\")\n",
    "    time_step = env.reset()\n",
    "    while not time_step.last():\n",
    "        player_id = time_step.observations[\"current_player\"]\n",
    "        if player_id == human_player:\n",
    "            agent_out = agents[human_player].step(time_step, is_evaluation=True)\n",
    "            #logging.info(\"\\n%s\", agent_out.probs)\n",
    "            #logging.info(\"\\n%s\", pretty_board(time_step))\n",
    "            action = command_line_action(time_step)\n",
    "        else:\n",
    "            agent_out = agents[1 - human_player].step(time_step, is_evaluation=True)\n",
    "            logging.info(\"\\n%s\", agent_out.probs)\n",
    "            action = agent_out.action\n",
    "            logging.info('Agent played: {}'.format(action+1))\n",
    "            \n",
    "            ### Trying to get the epsilon greedy probabilities see [6]\n",
    "            #obs = time_step.observations\n",
    "            #poss = agents[1 - human_player]._epsilon_greedy(tuple(obs['info_state'][1-human_player]),\n",
    "            #                                                obs['legal_actions'][1-human_player], 0.1)\n",
    "           \n",
    "        \n",
    "        logging.info('Player ID: %d', player_id)\n",
    "        #print(time_step.observations['info_state'][0])\n",
    "        #print(time_step.observations['info_state'][1])\n",
    "        #print(len(time_step.observations['info_state'][0]))\n",
    "        #print(env.observation_spec())\n",
    "        \n",
    "        state = time_step.observations['info_state'][player_id]\n",
    "        state = np.asarray(state)\n",
    "        \n",
    "        P_ob = np.where(state[points_ob_b:points_op_b] == 1)[0][0]\n",
    "        P_op = np.where(state[points_op_b:seq_b] == 1)[0][0]\n",
    "        logging.info('Points: P%d = %d P%d = %d',player_id, P_ob, 0 if player_id == 1 else 1, P_op)\n",
    "        \n",
    "        which = num_cards - np.sum(state[np.size(state) - num_cards:])\n",
    "        curr = np.where(state[int(seq_b + which*num_cards): int(seq_b + (which+1)*num_cards)] == 1)[0][0] + 1\n",
    "        logging.info('Point Card (Middle Card): %d', curr )\n",
    "        \n",
    "        # Can reach the current state of the game.\n",
    "        curr_state = env.get_state\n",
    "        print(curr_state)\n",
    "        \n",
    "        time_step = env.step([action])\n",
    "\n",
    "#logging.info(\"\\n%s\", pretty_board(time_step))\n",
    "\n",
    "    logging.info(\"End of game!\")\n",
    "    if time_step.rewards[human_player] > 0:\n",
    "        logging.info(\"You win\")\n",
    "    elif time_step.rewards[human_player] < 0:\n",
    "        logging.info(\"You lose\")\n",
    "    else:\n",
    "        logging.info(\"Draw\")\n",
    "    # Switch order of players\n",
    "    human_player = 1 - human_player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
