{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some important references to understand what is happening underneath:\n",
    "\n",
    "[1] For transforming game to equivalent turn-based https://github.com/deepmind/open_spiel/blob/master/open_spiel/game_transforms/turn_based_simultaneous_game.cc\n",
    "\n",
    "[2] For RL Environment https://github.com/deepmind/open_spiel/blob/master/open_spiel/python/rl_environment.py\n",
    "\n",
    "[3] For the game and state encodings https://github.com/deepmind/open_spiel/blob/master/open_spiel/games/goofspiel.cc\n",
    "\n",
    "[4] For policy https://github.com/deepmind/open_spiel/blob/master/open_spiel/python/policy.py\n",
    "\n",
    "Some examples\n",
    "\n",
    "[5] https://github.com/deepmind/open_spiel/tree/master/open_spiel/python/examples\n",
    "--> tic_tac_toe_qlearner.py, kuhn_nfsp.py, kuhn_policy_gradient.py, kuhn_poker_cfr.py\n",
    "\n",
    "Algorithms\n",
    "\n",
    "[6] https://github.com/deepmind/open_spiel/tree/master/open_spiel/python/algorithms\n",
    "--> tabular_qlearner.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps to Take: (Update 28.06.2020)\n",
    "\n",
    "Mark as (x) when done.\n",
    "\n",
    "* Change to turnbased. (x)\n",
    "* Adapt tabular q-learn. (x)\n",
    "* Understand and use rl environment. (x)\n",
    "* Try to get the learned policy as a table of probabilities.\n",
    "* Evaluate the the policy against random and other policies.\n",
    "* Try to implement more algorithms. (e.g. CFR)\n",
    "* Compare different algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ege/miniconda3/envs/gops/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "# Copyright 2019 DeepMind Technologies Ltd. All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"Tabular Q-Learner example on Tic Tac Toe.\n",
    "\n",
    "Two Q-Learning agents are trained by playing against each other. Then, the game\n",
    "can be played against the agents from the command line.\n",
    "\n",
    "After about 10**5 training episodes, the agents reach a good policy: win rate\n",
    "against random opponents is around 99% for player 0 and 92% for player 1.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "from absl import app\n",
    "from absl import flags\n",
    "import numpy as np\n",
    "from six.moves import input\n",
    "from six.moves import range\n",
    "\n",
    "from open_spiel.python import rl_environment\n",
    "from open_spiel.python.algorithms import random_agent\n",
    "from open_spiel.python.algorithms import tabular_qlearner\n",
    "import pyspiel\n",
    "\n",
    "# For gradient based methods\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "# For policy gradient\n",
    "from open_spiel.python import policy\n",
    "from open_spiel.python.algorithms import exploitability\n",
    "from open_spiel.python.algorithms import policy_gradient\n",
    "\n",
    "# For NFSP\n",
    "from open_spiel.python.algorithms import exploitability\n",
    "from open_spiel.python.algorithms import nfsp\n",
    "\n",
    "# For CFR\n",
    "from open_spiel.python.algorithms import cfr\n",
    "from open_spiel.python.algorithms import expected_game_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:test\n"
     ]
    }
   ],
   "source": [
    "# Needed for using logging in Notebook\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logging.debug(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Later modify it for goofspiel.\n",
    "\n",
    "def pretty_board(time_step):\n",
    "    \"\"\"Returns the board in `time_step` in a human readable format.\"\"\"\n",
    "    info_state = time_step.observations[\"info_state\"][0]\n",
    "    x_locations = np.nonzero(info_state[9:18])[0]\n",
    "    o_locations = np.nonzero(info_state[18:])[0]\n",
    "    board = np.full(3 * 3, \".\")\n",
    "    board[x_locations] = \"X\"\n",
    "    board[o_locations] = \"0\"\n",
    "    board = np.reshape(board, (3, 3))\n",
    "    return board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changed the format to print more user friendly on lines 8 and 12 the addition and subtraction by 1.\n",
    "def command_line_action(time_step):\n",
    "    \"\"\"Gets a valid action from the user on the command line.\"\"\"\n",
    "    current_player = time_step.observations[\"current_player\"]\n",
    "    legal_actions = time_step.observations[\"legal_actions\"][current_player]\n",
    "    cards_actions = [x+1 for x in legal_actions]\n",
    "    action = -1\n",
    "    while action not in legal_actions:\n",
    "        print(\"Choose an card to play from your hand {}:\".format(cards_actions))\n",
    "        sys.stdout.flush()\n",
    "        action_str = input()\n",
    "        try:\n",
    "            action = int(action_str) - 1\n",
    "        except ValueError:\n",
    "            continue\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_against_random_bots(env, trained_agents, random_agents, num_episodes):\n",
    "    \"\"\"Evaluates `trained_agents` against `random_agents` for `num_episodes`.\"\"\"\n",
    "    wins = np.zeros(2)\n",
    "    for player_pos in range(2):\n",
    "        if player_pos == 0:\n",
    "            cur_agents = [trained_agents[0], random_agents[1]]\n",
    "        else:\n",
    "            cur_agents = [random_agents[0], trained_agents[1]]\n",
    "        for _ in range(num_episodes):\n",
    "            time_step = env.reset()\n",
    "            while not time_step.last():\n",
    "                player_id = time_step.observations[\"current_player\"]\n",
    "                agent_output = cur_agents[player_id].step(time_step, is_evaluation=True)\n",
    "                time_step = env.step([agent_output.action])\n",
    "            if time_step.rewards[player_pos] > 0:\n",
    "                wins[player_pos] += 1\n",
    "    return wins / num_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_vs_one(env, red, blue, num_episodes):\n",
    "    \"\"\"Evaluates `trained_agents` against `random_agents` for `num_episodes`.\"\"\"\n",
    "    wins = np.zeros(2)\n",
    "    cur_agents = [red, blue]\n",
    "    for _ in range(num_episodes):\n",
    "        time_step = env.reset()\n",
    "        while not time_step.last():\n",
    "            player_id = time_step.observations[\"current_player\"]\n",
    "            agent_output = cur_agents[player_id].step(time_step, is_evaluation=True)\n",
    "            time_step = env.step([agent_output.action])\n",
    "        if time_step.rewards[0] > 0:\n",
    "            wins[0] += 1\n",
    "        else:\n",
    "            wins[1] += 1\n",
    "    return wins / num_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_vs_one(env, red_agents, blue_agents, num_episodes):\n",
    "    \"\"\"Evaluates `trained_agents` against `random_agents` for `num_episodes`.\"\"\"\n",
    "    wins = np.zeros(2)\n",
    "    for player_pos in range(2):\n",
    "        if player_pos == 0:\n",
    "            cur_agents = [red_agents[0], blue_agents[1]]\n",
    "        else:\n",
    "            cur_agents = [red_agents[0], blue_agents[1]]\n",
    "        for _ in range(num_episodes):\n",
    "            time_step = env.reset()\n",
    "            while not time_step.last():\n",
    "                player_id = time_step.observations[\"current_player\"]\n",
    "                agent_output = cur_agents[player_id].step(time_step, is_evaluation=True)\n",
    "                time_step = env.step([agent_output.action])\n",
    "            if time_step.rewards[player_pos] > 0:\n",
    "                wins[player_pos] += 1\n",
    "    return wins / num_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_players = 2\n",
    "num_cards = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# important indices for human playing results:\n",
    "\n",
    "# Usage: ob for observing, op for opponent, b for begining index, cu for current\n",
    "\n",
    "cu_player_b = 0\n",
    "ob_player_b = num_players\n",
    "points_ob_b = 2*num_players\n",
    "points_op_b = points_ob_b + int(num_cards*(num_cards+1)/2+1)\n",
    "seq_b = points_op_b + int(num_cards*(num_cards+1)/2+1)\n",
    "hand_ob_b = seq_b + num_cards**2\n",
    "hand_op_b = hand_ob_b + num_cards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is some of the algorithms being used, be careful to run only one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabular Q Learning\n",
    "This is the training of tabular Q-Learning agents.\n",
    "\n",
    "You can select if the algorithm should use observation tensor of information state tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = pyspiel.load_game_as_turn_based('goofspiel(imp_info=False,num_cards=4)')\n",
    "num_players = 2\n",
    "num_cards = 4\n",
    "\n",
    "# if observation_type == rl_environment.ObservationType.OBSERVATION then uses observation tensor however,\n",
    "# currently not working as we load the game beforehand as turn based. Maybe checking the code for game\n",
    "# transforms might help with using observation tensor. Still might not be necessary.\n",
    "env = rl_environment.Environment(game,observation_type=None)\n",
    "num_actions = env.action_spec()[\"num_actions\"]\n",
    "\n",
    "agents = [\n",
    "    tabular_qlearner.QLearner(player_id=idx, num_actions=num_actions)\n",
    "    for idx in range(num_players)\n",
    "]\n",
    "\n",
    "# random agents for evaluation\n",
    "random_agents = [\n",
    "    random_agent.RandomAgent(player_id=idx, num_actions=num_actions)\n",
    "    for idx in range(num_players)\n",
    "]\n",
    "\n",
    "# 1. Train the agents\n",
    "training_episodes = int(2e4)\n",
    "for cur_episode in range(training_episodes):\n",
    "    if cur_episode % int(1e4) == 0:\n",
    "        win_rates = eval_against_random_bots(env, agents, random_agents, 1000)\n",
    "        logging.info(\"Starting episode %s, win_rates %s\", cur_episode, win_rates)\n",
    "    time_step = env.reset()\n",
    "    while not time_step.last():\n",
    "        player_id = time_step.observations[\"current_player\"]\n",
    "        agent_output = agents[player_id].step(time_step)\n",
    "        time_step = env.step([agent_output.action])\n",
    "\n",
    "    # Episode is over, step all agents with final info state.\n",
    "    for agent in agents:\n",
    "        agent.step(time_step)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient - To Be Updated - Currently works\n",
    "\n",
    "### Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_integer(\"num_episodes\", int(2e4), \"Number of train episodes.\")\n",
    "flags.DEFINE_integer(\"eval_every\", int(1e4), \"Eval agents every x episodes.\")\n",
    "flags.DEFINE_enum(\"loss_str\", \"rpg\", [\"rpg\", \"qpg\", \"rm\"], \"PG loss to use.\")\n",
    "'''\n",
    "\n",
    "num_episodes = int(2e2)\n",
    "eval_every = int(1e2)\n",
    "loss_str = \"rpg\"\n",
    "\n",
    "class PolicyGradientPolicies(policy.Policy):\n",
    "    \"\"\"Joint policy to be evaluated.\"\"\"\n",
    "\n",
    "    def __init__(self, env, nfsp_policies):\n",
    "        game = env.game\n",
    "        player_ids = [0, 1]\n",
    "        super(PolicyGradientPolicies, self).__init__(game, player_ids)\n",
    "        self._policies = nfsp_policies\n",
    "        self._obs = {\"info_state\": [None, None], \"legal_actions\": [None, None]}\n",
    "\n",
    "    def action_probabilities(self, state, player_id=None):\n",
    "        cur_player = state.current_player()\n",
    "        legal_actions = state.legal_actions(cur_player)\n",
    "\n",
    "        self._obs[\"current_player\"] = cur_player\n",
    "        self._obs[\"info_state\"][cur_player] = (\n",
    "            state.information_state_tensor(cur_player))\n",
    "        self._obs[\"legal_actions\"][cur_player] = legal_actions\n",
    "\n",
    "        info_state = rl_environment.TimeStep(\n",
    "            observations=self._obs, rewards=None, discounts=None, step_type=None)\n",
    "\n",
    "        p = self._policies[cur_player].step(info_state, is_evaluation=True).probs\n",
    "        prob_dict = {action: p[action] for action in legal_actions}\n",
    "        return prob_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Training and interactive human playing at the same time.\n",
    "It really learns a probabilistic policy. Later try to seperate the training and interactive playing.\n",
    "tf.Session() should stay open for playing against a human.\n",
    "Also try to return the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.set_random_seed(\n",
    "    7\n",
    ")\n",
    "np.random.seed(7)\n",
    "import random as python_random\n",
    "python_random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using game instance: turn_based_simultaneous_game\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'PolicyGradient' object has no attribute 'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-bbe1c27d5c37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mexpl_policies_avg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPolicyGradientPolicies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ege/miniconda3/envs/gops/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, var_list, reshape, sharded, max_to_keep, keep_checkpoint_every_n_hours, name, restore_sequentially, saver_def, builder, defer_build, allow_empty, write_version, pad_step_number, save_relative_paths, filename)\u001b[0m\n\u001b[1;32m    834\u001b[0m           time.time() + self._keep_checkpoint_every_n_hours * 3600)\n\u001b[1;32m    835\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdefer_build\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 836\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    837\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_saver_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ege/miniconda3/envs/gops/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Use save/restore instead of build in eager mode.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_save\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_restore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_build_eager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_save\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_restore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ege/miniconda3/envs/gops/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36m_build\u001b[0;34m(self, checkpoint_path, build_save, build_restore)\u001b[0m\n\u001b[1;32m    884\u001b[0m           \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m           \u001b[0mbuild_save\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuild_save\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 886\u001b[0;31m           build_restore=build_restore)\n\u001b[0m\u001b[1;32m    887\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;31m# Since self._name is used as a name_scope by builder(), we are\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ege/miniconda3/envs/gops/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36m_build_internal\u001b[0;34m(self, names_to_saveables, reshape, sharded, max_to_keep, keep_checkpoint_every_n_hours, name, restore_sequentially, filename, build_save, build_restore)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m     saveables = saveable_object_util.validate_and_slice_inputs(\n\u001b[0;32m--> 490\u001b[0;31m         names_to_saveables)\n\u001b[0m\u001b[1;32m    491\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmax_to_keep\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m       \u001b[0mmax_to_keep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ege/miniconda3/envs/gops/lib/python3.6/site-packages/tensorflow/python/training/saving/saveable_object_util.py\u001b[0m in \u001b[0;36mvalidate_and_slice_inputs\u001b[0;34m(names_to_saveables)\u001b[0m\n\u001b[1;32m    340\u001b[0m   \"\"\"\n\u001b[1;32m    341\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames_to_saveables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m     \u001b[0mnames_to_saveables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop_list_to_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames_to_saveables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m   \u001b[0msaveables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ege/miniconda3/envs/gops/lib/python3.6/site-packages/tensorflow/python/training/saving/saveable_object_util.py\u001b[0m in \u001b[0;36mop_list_to_dict\u001b[0;34m(op_list, convert_variable_to_tensor)\u001b[0m\n\u001b[1;32m    236\u001b[0m   \u001b[0;31m# graph. Sorting the op_list ensures that the resulting graph is always\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m   \u001b[0;31m# constructed in a deterministic way:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m   \u001b[0mop_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m   \u001b[0mnames_to_saveables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ege/miniconda3/envs/gops/lib/python3.6/site-packages/tensorflow/python/training/saving/saveable_object_util.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    236\u001b[0m   \u001b[0;31m# graph. Sorting the op_list ensures that the resulting graph is always\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m   \u001b[0;31m# constructed in a deterministic way:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m   \u001b[0mop_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m   \u001b[0mnames_to_saveables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PolicyGradient' object has no attribute 'name'"
     ]
    }
   ],
   "source": [
    "game = pyspiel.load_game_as_turn_based('goofspiel(imp_info=False,num_cards=4)')\n",
    "num_players = 2\n",
    "\n",
    "env_configs = {\"players\": num_players}\n",
    "env = rl_environment.Environment(game, **env_configs)\n",
    "info_state_size = env.observation_spec()[\"info_state\"][0]\n",
    "num_actions = env.action_spec()[\"num_actions\"]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # pylint: disable=g-complex-comprehension\n",
    "    agents = [\n",
    "        policy_gradient.PolicyGradient(\n",
    "            sess,\n",
    "            idx,\n",
    "            info_state_size,\n",
    "            num_actions,\n",
    "            loss_str=loss_str,\n",
    "            hidden_layers_sizes=(128,)) for idx in range(num_players)\n",
    "    ]\n",
    "    expl_policies_avg = PolicyGradientPolicies(env, agents)\n",
    "\n",
    "    saver = tf.train.Saver(var_list=agents)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for ep in range(num_episodes):\n",
    "\n",
    "        if (ep + 1) % eval_every == 0:\n",
    "            losses = [agent.loss for agent in agents]\n",
    "            expl = exploitability.exploitability(env.game, expl_policies_avg)\n",
    "            msg = \"-\" * 80 + \"\\n\"\n",
    "            msg += \"{}: {}\\n{}\\n\".format(ep + 1, expl, losses)\n",
    "            logging.info(\"%s\", msg)\n",
    "\n",
    "        time_step = env.reset()\n",
    "        while not time_step.last():\n",
    "            player_id = time_step.observations[\"current_player\"]\n",
    "            agent_output = agents[player_id].step(time_step)\n",
    "            action_list = [agent_output.action]\n",
    "            time_step = env.step(action_list)\n",
    "\n",
    "        # Episode is over, step all agents with final info state.\n",
    "        for agent in agents:\n",
    "            agent.step(time_step)\n",
    "        \n",
    "    random_agents = [\n",
    "        random_agent.RandomAgent(player_id=idx, num_actions=num_actions)\n",
    "        for idx in range(2)\n",
    "    ]\n",
    "    #save_path = saver.save(sess, \"model_PG.ckpt\")\n",
    "    print(eval_against_random_bots(env, agents, random_agents, 1000))\n",
    "    saver.save(sess, 'models/model_PG.ckpt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model_PG.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model_PG.ckpt\n",
      "INFO:root:You are playing as ID 1\n",
      "INFO:root:\n",
      "[0.30873469 0.24189511 0.18298812 0.26638207]\n",
      "INFO:root:Agent played: 2\n",
      "INFO:root:Player ID: 0\n",
      "INFO:root:Points: P0 = 0 P1 = 0\n",
      "INFO:root:Point Card (Middle Card): 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.377 0.379]\n",
      "Partial joint action: \n",
      "P0 hand: 1 2 3 4 \n",
      "P1 hand: 1 2 3 4 \n",
      "Point card sequence: 2 \n",
      "Points: 0 0 \n",
      "\n",
      "Choose an card to play from your hand [1, 2, 3, 4]:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/home/ege/miniconda3/envs/gops/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ege/miniconda3/envs/gops/lib/python3.6/site-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 813\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    814\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ege/miniconda3/envs/gops/lib/python3.6/site-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/home/ege/miniconda3/envs/gops/lib/python3.6/site-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-47da0c0b53b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0;31m#logging.info(\"\\n%s\", agent_out.probs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0;31m#logging.info(\"\\n%s\", pretty_board(time_step))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcommand_line_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0magent_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mhuman_player\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_evaluation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-49898c30512a>\u001b[0m in \u001b[0;36mcommand_line_action\u001b[0;34m(time_step)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Choose an card to play from your hand {}:\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcards_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0maction_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_str\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ege/miniconda3/envs/gops/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    857\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         )\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ege/miniconda3/envs/gops/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:    \n",
    "    \n",
    "# 2. Play from the command line against the trained agent.\n",
    "\n",
    "    agents = [\n",
    "        policy_gradient.PolicyGradient(\n",
    "            sess,\n",
    "            idx,\n",
    "            info_state_size,\n",
    "            num_actions,\n",
    "            loss_str=loss_str,\n",
    "            hidden_layers_sizes=(128,)) for idx in range(num_players)\n",
    "    ]\n",
    "    expl_policies_avg = PolicyGradientPolicies(env, agents)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess, \"model_PG.ckpt\")\n",
    "    \n",
    "    random_agents = [\n",
    "        random_agent.RandomAgent(player_id=idx, num_actions=num_actions)\n",
    "        for idx in range(2)\n",
    "    ]\n",
    "    #save_path = saver.save(sess, \"model_PG.ckpt\")\n",
    "    print(eval_against_random_bots(env, agents, random_agents, 1000))\n",
    "    \n",
    "    human_player = 1\n",
    "    while True:\n",
    "        logging.info(\"You are playing as %s\", \"ID 1\" if human_player else \"ID 0\")\n",
    "        time_step = env.reset()\n",
    "        while not time_step.last():\n",
    "            player_id = time_step.observations[\"current_player\"]\n",
    "            if player_id == human_player:\n",
    "                agent_out = agents[human_player].step(time_step, is_evaluation=True)\n",
    "                #logging.info(\"\\n%s\", agent_out.probs)\n",
    "                #logging.info(\"\\n%s\", pretty_board(time_step))\n",
    "                action = command_line_action(time_step)\n",
    "            else:\n",
    "                agent_out = agents[1 - human_player].step(time_step, is_evaluation=True)\n",
    "                logging.info(\"\\n%s\", agent_out.probs)\n",
    "                action = agent_out.action\n",
    "                logging.info('Agent played: {}'.format(action+1))\n",
    "\n",
    "\n",
    "            logging.info('Player ID: %d', player_id)\n",
    "            #print(time_step.observations['info_state'][0])\n",
    "            #print(time_step.observations['info_state'][1])\n",
    "            #print(len(time_step.observations['info_state'][0]))\n",
    "            #print(env.observation_spec())\n",
    "\n",
    "            state = time_step.observations['info_state'][player_id]\n",
    "            state = np.asarray(state)\n",
    "\n",
    "            P_ob = np.where(state[points_ob_b:points_op_b] == 1)[0][0]\n",
    "            P_op = np.where(state[points_op_b:seq_b] == 1)[0][0]\n",
    "            logging.info('Points: P%d = %d P%d = %d',player_id, P_ob, 0 if player_id == 1 else 1, P_op)\n",
    "\n",
    "            which = num_cards - np.sum(state[np.size(state) - num_cards:])\n",
    "            curr = np.where(state[int(seq_b + which*num_cards): int(seq_b + (which+1)*num_cards)] == 1)[0][0] + 1\n",
    "            logging.info('Point Card (Middle Card): %d', curr )\n",
    "\n",
    "            # Can reach the current state of the game.\n",
    "            curr_state = env.get_state\n",
    "            print(curr_state)\n",
    "\n",
    "            time_step = env.step([action])\n",
    "\n",
    "    #logging.info(\"\\n%s\", pretty_board(time_step))\n",
    "\n",
    "        logging.info(\"End of game!\")\n",
    "        if time_step.rewards[human_player] > 0:\n",
    "            logging.info(\"You win\")\n",
    "        elif time_step.rewards[human_player] < 0:\n",
    "            logging.info(\"You lose\")\n",
    "        else:\n",
    "            logging.info(\"Draw\")\n",
    "        # Switch order of players\n",
    "        human_player = 1 - human_player"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Fictious Self-Play - To Be Updated - Currently Works\n",
    "\n",
    "### Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_integer(\"num_train_episodes\", int(3e6),\n",
    "                     \"Number of training episodes.\")\n",
    "flags.DEFINE_integer(\"eval_every\", 10000,\n",
    "                     \"Episode frequency at which the agents are evaluated.\")\n",
    "flags.DEFINE_list(\"hidden_layers_sizes\", [\n",
    "    128,\n",
    "], \"Number of hidden units in the avg-net and Q-net.\")\n",
    "flags.DEFINE_integer(\"replay_buffer_capacity\", int(2e5),\n",
    "                     \"Size of the replay buffer.\")\n",
    "flags.DEFINE_integer(\"reservoir_buffer_capacity\", int(2e6),\n",
    "                     \"Size of the reservoir buffer.\")\n",
    "flags.DEFINE_float(\"anticipatory_param\", 0.1,\n",
    "                   \"Prob of using the rl best response as episode policy.\")\n",
    "'''\n",
    "\n",
    "num_train_episodes = int(2e2)\n",
    "eval_every = 100\n",
    "hidden_layers_sizes = [128]\n",
    "replay_buffer_capacity = int(2e5)\n",
    "reservoir_buffer_capacity = int(2e6)\n",
    "anticipatory_param = 0.1\n",
    "\n",
    "\n",
    "class NFSPPolicies(policy.Policy):\n",
    "    \"\"\"Joint policy to be evaluated.\"\"\"\n",
    "\n",
    "    def __init__(self, env, nfsp_policies, mode):\n",
    "        game = env.game\n",
    "        player_ids = [0, 1]\n",
    "        super(NFSPPolicies, self).__init__(game, player_ids)\n",
    "        self._policies = nfsp_policies\n",
    "        self._mode = mode\n",
    "        self._obs = {\"info_state\": [None, None], \"legal_actions\": [None, None]}\n",
    "\n",
    "    def action_probabilities(self, state, player_id=None):\n",
    "        cur_player = state.current_player()\n",
    "        legal_actions = state.legal_actions(cur_player)\n",
    "\n",
    "        self._obs[\"current_player\"] = cur_player\n",
    "        self._obs[\"info_state\"][cur_player] = (\n",
    "            state.information_state_tensor(cur_player))\n",
    "        self._obs[\"legal_actions\"][cur_player] = legal_actions\n",
    "\n",
    "        info_state = rl_environment.TimeStep(\n",
    "            observations=self._obs, rewards=None, discounts=None, step_type=None)\n",
    "\n",
    "        with self._policies[cur_player].temp_mode_as(self._mode):\n",
    "            p = self._policies[cur_player].step(info_state, is_evaluation=True).probs\n",
    "        prob_dict = {action: p[action] for action in legal_actions}\n",
    "        return prob_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using game instance: turn_based_simultaneous_game\n",
      "INFO:root:Losses: [(1.1790241, 0.2361854), (1.2848668, 0.2319324)]\n",
      "INFO:root:[10000] Exploitability AVG 0.7879914344015732\n",
      "INFO:root:_____________________________________________\n",
      "INFO:root:Losses: [(1.0671444, 0.28515267), (1.0453298, 0.20903808)]\n",
      "INFO:root:[20000] Exploitability AVG 0.8318555553113312\n",
      "INFO:root:_____________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.417 0.418]\n"
     ]
    }
   ],
   "source": [
    "game = pyspiel.load_game_as_turn_based('goofspiel(imp_info=False,num_cards=4)')\n",
    "num_players = 2\n",
    "\n",
    "env_configs = {\"players\": num_players}\n",
    "env = rl_environment.Environment(game, **env_configs)\n",
    "info_state_size = env.observation_spec()[\"info_state\"][0]\n",
    "num_actions = env.action_spec()[\"num_actions\"]\n",
    "\n",
    "hidden_layers_sizes = [int(l) for l in hidden_layers_sizes]\n",
    "kwargs = {\n",
    "    \"replay_buffer_capacity\": replay_buffer_capacity,\n",
    "    \"epsilon_decay_duration\": num_train_episodes,\n",
    "    \"epsilon_start\": 0.06,\n",
    "    \"epsilon_end\": 0.001,\n",
    "}\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # pylint: disable=g-complex-comprehension\n",
    "    agents = [\n",
    "        nfsp.NFSP(sess, idx, info_state_size, num_actions, hidden_layers_sizes,\n",
    "                  reservoir_buffer_capacity, anticipatory_param,\n",
    "                  **kwargs) for idx in range(num_players)\n",
    "    ]\n",
    "    expl_policies_avg = NFSPPolicies(env, agents, nfsp.MODE.average_policy)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for ep in range(num_train_episodes):\n",
    "        if (ep + 1) % eval_every == 0:\n",
    "            losses = [agent.loss for agent in agents]\n",
    "            logging.info(\"Losses: %s\", losses)\n",
    "            expl = exploitability.exploitability(env.game, expl_policies_avg)\n",
    "            logging.info(\"[%s] Exploitability AVG %s\", ep + 1, expl)\n",
    "            logging.info(\"_____________________________________________\")\n",
    "\n",
    "        time_step = env.reset()\n",
    "        while not time_step.last():\n",
    "            player_id = time_step.observations[\"current_player\"]\n",
    "            agent_output = agents[player_id].step(time_step)\n",
    "            action_list = [agent_output.action]\n",
    "            time_step = env.step(action_list)\n",
    "\n",
    "          # Episode is over, step all agents with final info state.\n",
    "        for agent in agents:\n",
    "            agent.step(time_step)\n",
    "            \n",
    "    print(eval_against_random_bots(env, agents, random_agents, 1000))\n",
    "    saver.save(sess, 'models/model_NFSP.ckpt')        \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing Against a Human"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play Against a Human\n",
    "\n",
    "For acquiring important information from the time_step.observations dictionary you can select the info_state, then select the necessary information.\n",
    "\n",
    "n = number of cards\n",
    "p = 2, number of players\n",
    "\n",
    "If using information state tensor with imp_info = False:\n",
    "\n",
    "First 2 bits show which player is the current player.\n",
    "\n",
    "Next 2 bits show which player the current observation belongs to.\n",
    "\n",
    "(See line 178 in [1])\n",
    "\n",
    "Next $ \\frac{n \\cdot (n+1)}{2} + 1 $ bits show the points of the player observing.\n",
    "\n",
    "Next $ \\frac{n \\cdot (n+1)}{2} + 1 $ bits show the points of the opponent.\n",
    "\n",
    "Next $ n^{2} $ bits show the sequence of the point cards, namely the cards that are opened in the middle.\n",
    "\n",
    "Last $ n \\cdot p $ bits show the hand of each player ordered from the perspective of the observing player (observing player has the first n bits and opponent has the last n bits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2. Play from the command line against the trained agent.\n",
    "human_player = 1\n",
    "while True:\n",
    "    logging.info(\"You are playing as %s\", \"ID 1\" if human_player else \"ID 0\")\n",
    "    time_step = env.reset()\n",
    "    while not time_step.last():\n",
    "        player_id = time_step.observations[\"current_player\"]\n",
    "        if player_id == human_player:\n",
    "            agent_out = agents[human_player].step(time_step, is_evaluation=True)\n",
    "            #logging.info(\"\\n%s\", agent_out.probs)\n",
    "            #logging.info(\"\\n%s\", pretty_board(time_step))\n",
    "            action = command_line_action(time_step)\n",
    "        else:\n",
    "            agent_out = agents[1 - human_player].step(time_step, is_evaluation=True)\n",
    "            logging.info(\"\\n%s\", agent_out.probs)\n",
    "            action = agent_out.action\n",
    "            logging.info('Agent played: {}'.format(action+1))\n",
    "            \n",
    "            ### Trying to get the epsilon greedy probabilities see [6]\n",
    "            #obs = time_step.observations\n",
    "            #poss = agents[1 - human_player]._epsilon_greedy(tuple(obs['info_state'][1-human_player]),\n",
    "            #                                                obs['legal_actions'][1-human_player], 0.1)\n",
    "           \n",
    "        \n",
    "        logging.info('Player ID: %d', player_id)\n",
    "        #print(time_step.observations['info_state'][0])\n",
    "        #print(time_step.observations['info_state'][1])\n",
    "        #print(len(time_step.observations['info_state'][0]))\n",
    "        #print(env.observation_spec())\n",
    "        \n",
    "        state = time_step.observations['info_state'][player_id]\n",
    "        state = np.asarray(state)\n",
    "        \n",
    "        P_ob = np.where(state[points_ob_b:points_op_b] == 1)[0][0]\n",
    "        P_op = np.where(state[points_op_b:seq_b] == 1)[0][0]\n",
    "        logging.info('Points: P%d = %d P%d = %d',player_id, P_ob, 0 if player_id == 1 else 1, P_op)\n",
    "        \n",
    "        which = num_cards - np.sum(state[np.size(state) - num_cards:])\n",
    "        curr = np.where(state[int(seq_b + which*num_cards): int(seq_b + (which+1)*num_cards)] == 1)[0][0] + 1\n",
    "        logging.info('Point Card (Middle Card): %d', curr )\n",
    "        \n",
    "        # Can reach the current state of the game.\n",
    "        curr_state = env.get_state\n",
    "        print(curr_state)\n",
    "        \n",
    "        time_step = env.step([action])\n",
    "\n",
    "#logging.info(\"\\n%s\", pretty_board(time_step))\n",
    "\n",
    "    logging.info(\"End of game!\")\n",
    "    if time_step.rewards[human_player] > 0:\n",
    "        logging.info(\"You win\")\n",
    "    elif time_step.rewards[human_player] < 0:\n",
    "        logging.info(\"You lose\")\n",
    "    else:\n",
    "        logging.info(\"Draw\")\n",
    "    # Switch order of players\n",
    "    human_player = 1 - human_player"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Free-For-All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using game instance: turn_based_simultaneous_game\n",
      "INFO:root:Losses: [(0.62956035, 0.22984585), (0.7728914, 0.2794257)]\n",
      "INFO:root:[10000] Exploitability AVG 0.779707100405488\n",
      "INFO:root:_____________________________________________\n",
      "INFO:root:Losses: [(1.2178802, 0.23888847), (1.0064793, 0.24753928)]\n",
      "INFO:root:[10000] Exploitability AVG 0.7974192481059763\n",
      "INFO:root:_____________________________________________\n",
      "INFO:root:Losses: [(0.58345366, 0.27146232), (0.4189149, 0.3119984)]\n",
      "INFO:root:[20000] Exploitability AVG 0.782226474027669\n",
      "INFO:root:_____________________________________________\n",
      "INFO:root:Losses: [(0.9345692, 0.27358592), (0.74967015, 0.20577604)]\n",
      "INFO:root:[20000] Exploitability AVG 0.8394422270311641\n",
      "INFO:root:_____________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.396 0.428]\n",
      "[0.365 0.399]\n"
     ]
    }
   ],
   "source": [
    "game = pyspiel.load_game_as_turn_based('goofspiel(imp_info=False,num_cards=4)')\n",
    "num_players = 2\n",
    "\n",
    "env_configs = {\"players\": num_players}\n",
    "env = rl_environment.Environment(game, **env_configs)\n",
    "info_state_size = env.observation_spec()[\"info_state\"][0]\n",
    "num_actions = env.action_spec()[\"num_actions\"]\n",
    "\n",
    "hidden_layers_sizes = [int(l) for l in hidden_layers_sizes]\n",
    "kwargs = {\n",
    "    \"replay_buffer_capacity\": replay_buffer_capacity,\n",
    "    \"epsilon_decay_duration\": num_train_episodes,\n",
    "    \"epsilon_start\": 0.06,\n",
    "    \"epsilon_end\": 0.001,\n",
    "}\n",
    "\n",
    "num_train_episodes = int(2e4)\n",
    "eval_every = int(1e4)\n",
    "hidden_layers_sizes = [128]\n",
    "replay_buffer_capacity = int(2e5)\n",
    "reservoir_buffer_capacity = int(2e6)\n",
    "anticipatory_param = 0.1\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # pylint: disable=g-complex-comprehension\n",
    "    pg_agents = [\n",
    "        policy_gradient.PolicyGradient(\n",
    "            sess,\n",
    "            idx,\n",
    "            info_state_size,\n",
    "            num_actions,\n",
    "            loss_str=loss_str,\n",
    "            hidden_layers_sizes=(128,)) for idx in range(num_players)\n",
    "    ]\n",
    "    \n",
    "    nfsp_agents = [\n",
    "        nfsp.NFSP(sess, idx, info_state_size, num_actions, hidden_layers_sizes,\n",
    "                  reservoir_buffer_capacity, anticipatory_param,\n",
    "                  **kwargs) for idx in range(num_players)\n",
    "    ]\n",
    "    \n",
    "    random_agents = [\n",
    "        random_agent.RandomAgent(player_id=idx, num_actions=num_actions)\n",
    "        for idx in range(2)\n",
    "    ]\n",
    "    \n",
    "    pg_expl_policies_avg = PolicyGradientPolicies(env, pg_agents)\n",
    "    nfsp_expl_policies_avg = NFSPPolicies(env, nfsp_agents, nfsp.MODE.average_policy)\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for ep in range(num_train_episodes):\n",
    "        if (ep + 1) % eval_every == 0:\n",
    "            losses = [agent.loss for agent in pg_agents]\n",
    "            logging.info(\"Losses: %s\", losses)\n",
    "            expl = exploitability.exploitability(env.game, pg_expl_policies_avg)\n",
    "            logging.info(\"[%s] Exploitability AVG %s\", ep + 1, expl)\n",
    "            logging.info(\"_____________________________________________\")\n",
    "            \n",
    "            losses = [agent.loss for agent in nfsp_agents]\n",
    "            logging.info(\"Losses: %s\", losses)\n",
    "            expl = exploitability.exploitability(env.game, nfsp_expl_policies_avg)\n",
    "            logging.info(\"[%s] Exploitability AVG %s\", ep + 1, expl)\n",
    "            logging.info(\"_____________________________________________\")\n",
    "\n",
    "        time_step = env.reset()\n",
    "        while not time_step.last():\n",
    "            player_id = time_step.observations[\"current_player\"]\n",
    "            agent_output = pg_agents[player_id].step(time_step)\n",
    "            action_list = [agent_output.action]\n",
    "            time_step = env.step(action_list)\n",
    "\n",
    "          # Episode is over, step all agents with final info state.\n",
    "        for agent in pg_agents:\n",
    "            agent.step(time_step)\n",
    "            \n",
    "        time_step = env.reset()\n",
    "        while not time_step.last():\n",
    "            player_id = time_step.observations[\"current_player\"]\n",
    "            agent_output = nfsp_agents[player_id].step(time_step)\n",
    "            action_list = [agent_output.action]\n",
    "            time_step = env.step(action_list)\n",
    "\n",
    "          # Episode is over, step all agents with final info state.\n",
    "        for agent in nfsp_agents:\n",
    "            agent.step(time_step)\n",
    "            \n",
    "    print(eval_against_random_bots(env, pg_agents, random_agents, 1000))\n",
    "    print(eval_against_random_bots(env, nfsp_agents, random_agents, 1000))\n",
    "    saver.save(sess, 'models/model_algos.ckpt')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/model_algos.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/model_algos.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.409 0.418]\n",
      "[0.429 0.433]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:You are playing as ID 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.41  0.426]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'agents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-7840a25c738b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcommand_line_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                 \u001b[0magent_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mhuman_player\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_evaluation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m                 \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n%s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'agents' is not defined"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:    \n",
    "    \n",
    "# 2. Play from the command line against the trained agent.\n",
    "\n",
    "    pg_agents = [\n",
    "        policy_gradient.PolicyGradient(\n",
    "            sess,\n",
    "            idx,\n",
    "            info_state_size,\n",
    "            num_actions,\n",
    "            loss_str=loss_str,\n",
    "            hidden_layers_sizes=(128,)) for idx in range(num_players)\n",
    "    ]\n",
    "    \n",
    "    nfsp_agents = [\n",
    "        nfsp.NFSP(sess, idx, info_state_size, num_actions, hidden_layers_sizes,\n",
    "                  reservoir_buffer_capacity, anticipatory_param,\n",
    "                  **kwargs) for idx in range(num_players)\n",
    "    ]\n",
    "    \n",
    "    random_agents = [\n",
    "        random_agent.RandomAgent(player_id=idx, num_actions=num_actions)\n",
    "        for idx in range(2)\n",
    "    ]\n",
    "    \n",
    "    pg_expl_policies_avg = PolicyGradientPolicies(env, pg_agents)\n",
    "    nfsp_expl_policies_avg = NFSPPolicies(env, nfsp_agents, nfsp.MODE.average_policy)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess, \"models/model_algos.ckpt\")\n",
    "    \n",
    "    \n",
    "    #save_path = saver.save(sess, \"model_PG.ckpt\")\n",
    "    print(eval_against_random_bots(env, pg_agents, random_agents, 1000))\n",
    "    print(eval_against_random_bots(env, nfsp_agents, random_agents, 1000))\n",
    "    print(one_vs_one(env, pg_agents, nfsp_agents, 1000))\n",
    "    \n",
    "    \n",
    "    human_player = 1\n",
    "    while True:\n",
    "        logging.info(\"You are playing as %s\", \"ID 1\" if human_player else \"ID 0\")\n",
    "        time_step = env.reset()\n",
    "        while not time_step.last():\n",
    "            player_id = time_step.observations[\"current_player\"]\n",
    "            if player_id == human_player:\n",
    "                agent_out = agents[human_player].step(time_step, is_evaluation=True)\n",
    "                #logging.info(\"\\n%s\", agent_out.probs)\n",
    "                #logging.info(\"\\n%s\", pretty_board(time_step))\n",
    "                action = command_line_action(time_step)\n",
    "            else:\n",
    "                agent_out = agents[1 - human_player].step(time_step, is_evaluation=True)\n",
    "                logging.info(\"\\n%s\", agent_out.probs)\n",
    "                action = agent_out.action\n",
    "                logging.info('Agent played: {}'.format(action+1))\n",
    "\n",
    "\n",
    "            logging.info('Player ID: %d', player_id)\n",
    "            #print(time_step.observations['info_state'][0])\n",
    "            #print(time_step.observations['info_state'][1])\n",
    "            #print(len(time_step.observations['info_state'][0]))\n",
    "            #print(env.observation_spec())\n",
    "\n",
    "            state = time_step.observations['info_state'][player_id]\n",
    "            state = np.asarray(state)\n",
    "\n",
    "            P_ob = np.where(state[points_ob_b:points_op_b] == 1)[0][0]\n",
    "            P_op = np.where(state[points_op_b:seq_b] == 1)[0][0]\n",
    "            logging.info('Points: P%d = %d P%d = %d',player_id, P_ob, 0 if player_id == 1 else 1, P_op)\n",
    "\n",
    "            which = num_cards - np.sum(state[np.size(state) - num_cards:])\n",
    "            curr = np.where(state[int(seq_b + which*num_cards): int(seq_b + (which+1)*num_cards)] == 1)[0][0] + 1\n",
    "            logging.info('Point Card (Middle Card): %d', curr )\n",
    "\n",
    "            # Can reach the current state of the game.\n",
    "            curr_state = env.get_state\n",
    "            print(curr_state)\n",
    "\n",
    "            time_step = env.step([action])\n",
    "\n",
    "    #logging.info(\"\\n%s\", pretty_board(time_step))\n",
    "\n",
    "        logging.info(\"End of game!\")\n",
    "        if time_step.rewards[human_player] > 0:\n",
    "            logging.info(\"You win\")\n",
    "        elif time_step.rewards[human_player] < 0:\n",
    "            logging.info(\"You lose\")\n",
    "        else:\n",
    "            logging.info(\"Draw\")\n",
    "        # Switch order of players\n",
    "        human_player = 1 - human_player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = pyspiel.load_game_as_turn_based('goofspiel(imp_info=False,num_cards=4)')\n",
    "num_players = 2\n",
    "\n",
    "env_configs = {\"players\": num_players}\n",
    "env = rl_environment.Environment(game, **env_configs)\n",
    "info_state_size = env.observation_spec()[\"info_state\"][0]\n",
    "num_actions = env.action_spec()[\"num_actions\"]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # pylint: disable=g-complex-comprehension\n",
    "    agents = [\n",
    "        policy_gradient.PolicyGradient(\n",
    "            sess,\n",
    "            idx,\n",
    "            info_state_size,\n",
    "            num_actions,\n",
    "            loss_str=loss_str,\n",
    "            hidden_layers_sizes=(128,)) for idx in range(num_players)\n",
    "    ]\n",
    "    expl_policies_avg = PolicyGradientPolicies(env, agents)\n",
    "\n",
    "    saver = tf.train.Saver(var_list=agents)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for ep in range(num_episodes):\n",
    "\n",
    "        if (ep + 1) % eval_every == 0:\n",
    "            losses = [agent.loss for agent in agents]\n",
    "            expl = exploitability.exploitability(env.game, expl_policies_avg)\n",
    "            msg = \"-\" * 80 + \"\\n\"\n",
    "            msg += \"{}: {}\\n{}\\n\".format(ep + 1, expl, losses)\n",
    "            logging.info(\"%s\", msg)\n",
    "\n",
    "        time_step = env.reset()\n",
    "        while not time_step.last():\n",
    "            player_id = time_step.observations[\"current_player\"]\n",
    "            agent_output = agents[player_id].step(time_step)\n",
    "            action_list = [agent_output.action]\n",
    "            time_step = env.step(action_list)\n",
    "\n",
    "        # Episode is over, step all agents with final info state.\n",
    "        for agent in agents:\n",
    "            agent.step(time_step)\n",
    "        \n",
    "    random_agents = [\n",
    "        random_agent.RandomAgent(player_id=idx, num_actions=num_actions)\n",
    "        for idx in range(2)\n",
    "    ]\n",
    "    #save_path = saver.save(sess, \"model_PG.ckpt\")\n",
    "    print(eval_against_random_bots(env, agents, random_agents, 1000))\n",
    "    saver.save(sess, 'models/model_PG.ckpt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
