{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspiel\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = pyspiel.load_game(\"goofspiel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layout of `state.information_state_tensor(player_id)` in Goofspiel:\n",
    "\n",
    " 1st 92 values define the one-hot vector representing player 0's score.\n",
    " \n",
    " Following 92 values define the one-hot vector representing player 1's score.\n",
    " \n",
    " Following 13 values define the 1st point card if it has been opened, else they are all 0.\n",
    " \n",
    " There are 12*13 more values, each defined in the same way.\n",
    " \n",
    " Final 26 values define players' hands.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, a deterministic agent who always bids the card which has the value equal to the card in the middle, plays against a random agent who always bids one of his remaining cards uniform at random. A total of `num_episodes` games are played. The win rate of the deterministic agent is reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deterministic agent win rate = 0.957\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 1000\n",
    "deterministic_agent_returns = 0\n",
    "random_agent_returns = 0\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    state = game.new_initial_state()\n",
    "    turn = 1\n",
    "    \n",
    "    while not state.is_terminal():\n",
    "\n",
    "        legal_actions = state.legal_actions()\n",
    "\n",
    "        if state.is_chance_node():\n",
    "            # Chance node: sample an outcome\n",
    "            outcomes = state.chance_outcomes()\n",
    "            num_actions = len(outcomes)\n",
    "            action_list, prob_list = zip(*outcomes)\n",
    "            action = np.random.choice(action_list, p=prob_list)\n",
    "            state.apply_action(action)\n",
    "\n",
    "        elif state.is_simultaneous_node():\n",
    "            # Simultaneous node: sample actions for all players.\n",
    "            chosen_actions = [\n",
    "              random.choice(state.legal_actions(pid))\n",
    "              for pid in range(game.num_players())\n",
    "            ]\n",
    "            chosen_actions[0] = current_point_card(state.information_state_tensor(0), turn) - 1\n",
    "            state.apply_actions(chosen_actions)\n",
    "\n",
    "            turn += 1\n",
    "            \n",
    "    returns = state.returns()\n",
    "    deterministic_agent_returns += np.max([0, returns[0]])\n",
    "    random_agent_returns += np.max([0, returns[1]])\n",
    "\n",
    "print(\"deterministic agent win rate = {}\".format(deterministic_agent_returns/num_episodes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def current_point_card(info_state, turn):\n",
    "    '''\n",
    "    Returns the value of the last opened prize card.\n",
    "    '''\n",
    "    one_hot = info_state[184+(turn-1)*13:184+turn*13]\n",
    "    return np.argmax(np.array(one_hot))+1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gops] *",
   "language": "python",
   "name": "conda-env-gops-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
